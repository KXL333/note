# 20191209

1. [Docker Network—Bridge 模式](https://www.cnkirito.moe/docker-network-bridge/)


2. [k8s安装]

/proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1
  echo 1 >  /proc/sys/net/bridge/bridge-nf-call-iptables

/proc/sys/net/ipv4/ip_forward contents are not set to 1
  echo 1 > /proc/sys/net/ipv4/ip_forward


running with swap on is not supported. Please disable swap
  # 关闭Swap，机器重启后不生效
swapoff -a

# 修改/etc/fstab永久关闭Swap
cp -p /etc/fstab /etc/fstab.bak$(date '+%Y%m%d%H%M%S')
# Redhat
sed -i "s/\/dev\/mapper\/rhel-swap/\#\/dev\/mapper\/rhel-swap/g" /etc/fstab
# CentOS
sed -i "s/\/dev\/mapper\/centos-swap/\#\/dev\/mapper\/centos-swap/g" /etc/fstab
# 修改后重新挂载全部挂载点
mount -a

# 查看Swap
free -m
cat /proc/swaps
————————————————
版权声明：本文为CSDN博主「nklinsirui」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/nklinsirui/article/details/80855415

/var/lib/etcd is not empty
rm -rf /var/lib/etcd


kubeadm init
W1209 20:31:15.048529    3631 version.go:101] could not fetch a Kubernetes version from the internet: unable to get URL "https://dl.k8s.io/release/stable-1.txt": Get https://dl.k8s.io/release/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
W1209 20:31:15.048679    3631 version.go:102] falling back to the local client version: v1.16.3
[init] Using Kubernetes version: v1.16.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-apiserver:v1.16.3: output: Trying to pull repository k8s.gcr.io/kube-apiserver ...
Get https://k8s.gcr.io/v1/_ping: dial tcp 64.233.189.82:443: i/o timeout
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-controller-manager:v1.16.3: output: Trying to pull repository k8s.gcr.io/kube-controller-manager ...
Get https://k8s.gcr.io/v1/_ping: dial tcp 64.233.189.82:443: i/o timeout
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-scheduler:v1.16.3: output: Trying to pull repository k8s.gcr.io/kube-scheduler ...
Get https://k8s.gcr.io/v1/_ping: dial tcp 108.177.97.82:443: i/o timeout
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-proxy:v1.16.3: output: Trying to pull repository k8s.gcr.io/kube-proxy ...
Get https://k8s.gcr.io/v1/_ping: dial tcp 108.177.97.82:443: i/o timeout
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/pause:3.1: output: Trying to pull repository k8s.gcr.io/pause ...
Get https://k8s.gcr.io/v1/_ping: dial tcp 108.177.97.82:443: i/o timeout
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/etcd:3.3.15-0: output: Trying to pull repository k8s.gcr.io/etcd ...
Get https://k8s.gcr.io/v1/_ping: dial tcp 108.177.97.82:443: i/o timeout
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/coredns:1.6.2: output: Trying to pull repository k8s.gcr.io/coredns ...
Get https://k8s.gcr.io/v1/_ping: dial tcp 108.177.97.82:443: i/o timeout
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher


docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.2.24
------
k8s.gcr.io/kube-apiserver:v1.16.3
k8s.gcr.io/kube-controller-manager:v1.16.3
k8s.gcr.io/kube-scheduler:v1.16.3
k8s.gcr.io/kube-proxy:v1.16.3
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.2.24
3.3.15-0
k8s.gcr.io/coredns:1.6.2
-------

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.16.3
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.16.3 k8s.gcr.io/kube-apiserver:v1.16.3

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.16.3
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.16.3 k8s.gcr.io/kube-controller-manager:v1.16.3

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.16.3
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.16.3 k8s.gcr.io/kube-scheduler:v1.16.3

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.16.3
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.16.3 k8s.gcr.io/kube-proxy:v1.16.3

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 k8s.gcr.io/pause:3.1

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.3.15-0
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.3.15-0 k8s.gcr.io/etcd:3.3.15-0

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.6.2
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.6.2 k8s.gcr.io/coredns:1.6.2


docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1

发现是SELinux的原因（参考这个文章得到思路）
解决方案如下：
[root@centos7vm ~]# sestatus
SELinux status: enabled
于是关闭selinux(setenforce 0 在我的centos7.6上不起作用)
vim /etc/selinux/config
将SELINUX=enforcing改为SELINUX=disabled 
设置后需要重启才能生效
[root@centos7vm ~]# sestatus
SELinux status: disabled
————————————————
版权声明：本文为CSDN博主「lampNick」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/scun_cg/article/details/89517550


#kubeadm reset
#kubeadm init 


sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf


failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kubelet config file "/var/lib/kubelet/config.yaml", error: open /var/lib/kubelet/config.yaml: no such file or directory



修改或新增 /etc/docker/daemon.json

# vi /etc/docker/daemon.json

{

"registry-mirrors": ["http://hub-mirror.c.163.com"]

}

systemctl restart docker.service
————————————————
版权声明：本文为CSDN博主「superwind」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/jixuju/article/details/80158493


Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.211.55.5:6443 --token mliaho.zx7ds8v460g51ebp \
    --discovery-token-ca-cert-hash sha256:3e00904bf1fcd7d80f4fc6469076ab4bed22263b68fe32173f1f0dad0f5d87f5



    kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"


    kubectl get pods --all-namespaces



    kubectl taint nodes --all node-role.kubernetes.io/master-


    kubectl --namespace=kube-system describe pod coredns-5644d7b6d9-674t5



    kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml





    https://www.jianshu.com/p/c6d560d12d50


ksys edit svc kubernetes-dashboard


https://www.cnblogs.com/harlanzhang/p/10045975.html

kubectl describe secret dashboard-admin-token-n2tfx -n kube-system




Name:         dashboard-admin-token-n2tfx
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: dashboard-admin
              kubernetes.io/service-account.uid: ab5b60bf-7932-419e-a2e6-832361c1b1fc

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6ImJkZ2hSLWJWNTRYWW0wRWx1bFIyZG5Bd1hNSHBVb1V3UXZLWTh1dGxmY1EifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tbjJ0ZngiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYWI1YjYwYmYtNzkzMi00MTllLWEyZTYtODMyMzYxYzFiMWZjIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.OeW0F1hlKSbqUfn8DnX9w11hvRodPSKqPdalwMw6iWTznkoOqWbKjsYKnvO5F1BO7sjITwtAYLMPRypEpYXUC2v5qxn0VgwTiHzELYgZBFF0Veeq4f2VdYLZ_-C7v6wIParHRGDP9fy0Kvkmb7Xzzoykq4I9C1M75-uz0Ok2EvwCkOYfQS0FoU1lzJvpHtwo6PjzUW89uhrqmxWoB5A0CpZwTgPUNie-KYEubuHd-_hshKs92UC1FSoohjbMaFpyAKdjt63jgI228l21Tslr-X47gWc5aHke93mpcHbtGlmgmQJvs-FSLey5YgMoKsscxy6YJFo3LLUOvDAQufTZRQ





Name:         kubernetes-dashboard-token-m6ctc
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: kubernetes-dashboard
              kubernetes.io/service-account.uid: 79d04f07-9f59-44f6-b9b7-3b65a0c35b21

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6ImJkZ2hSLWJWNTRYWW0wRWx1bFIyZG5Bd1hNSHBVb1V3UXZLWTh1dGxmY1EifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi1tNmN0YyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6Ijc5ZDA0ZjA3LTlmNTktNDRmNi1iOWI3LTNiNjVhMGMzNWIyMSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.dXvdlVNApui9LEGvU1JhDjswoWuOnICIN_qBc46yEQgbmUd4zSHuHV_iWozIZip3PxkxRPqGlFY3zexJx2NTtr3lZr92vG5Z8Aeaqu9doIiplSU_gqL7lIX550V0QSwrjQYsUvKQOlW27QmzVhbDqGj3kiULdamp_f8kUN4_uT9cB73_32A_s6lYZqgpTlElbaBK9Nwv9ajW23Jdy-8oT4pFWUdYl29MPIbNJxppM746JUge6w59hmQfAnz2WDDg9YKOA1rQt5jYpHun6yDoT7QQTob4PwaWlmE56EyBQ7MVgXeSsZLcpmoTCB-z7S1FaEeFOROEiiaVSv3YSVVt6A

[root@kubernetes01 pki]# kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin
clusterrolebinding.rbac.authorization.k8s.io/dashboard-cluster-admin created


Name:         dashboard-admin-token-n2tfx
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: dashboard-admin
              kubernetes.io/service-account.uid: ab5b60bf-7932-419e-a2e6-832361c1b1fc

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6ImJkZ2hSLWJWNTRYWW0wRWx1bFIyZG5Bd1hNSHBVb1V3UXZLWTh1dGxmY1EifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tbjJ0ZngiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYWI1YjYwYmYtNzkzMi00MTllLWEyZTYtODMyMzYxYzFiMWZjIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.OeW0F1hlKSbqUfn8DnX9w11hvRodPSKqPdalwMw6iWTznkoOqWbKjsYKnvO5F1BO7sjITwtAYLMPRypEpYXUC2v5qxn0VgwTiHzELYgZBFF0Veeq4f2VdYLZ_-C7v6wIParHRGDP9fy0Kvkmb7Xzzoykq4I9C1M75-uz0Ok2EvwCkOYfQS0FoU1lzJvpHtwo6PjzUW89uhrqmxWoB5A0CpZwTgPUNie-KYEubuHd-_hshKs92UC1FSoohjbMaFpyAKdjt63jgI228l21Tslr-X47gWc5aHke93mpcHbtGlmgmQJvs-FSLey5YgMoKsscxy6YJFo3LLUOvDAQufTZRQ